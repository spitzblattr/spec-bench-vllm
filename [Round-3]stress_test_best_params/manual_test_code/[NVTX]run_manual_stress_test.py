#import subprocess
import time
import requests
import os
import os
import pycuda.driver as cuda
pycuda_init_done = False  # global Flag
from multiprocessing import Process

# ------------------------------------------------------------
# max-concurr = inf  
# speculative_max_model_len: Optional[int]. æ³¨æ„è¿™ä¸ªå€¼ä¼šè¦†ç›–å°è‰ç¨¿æ¨¡å‹çš„ã€context lengthã€‘(ä¹Ÿå°±æ˜¯ prompt+output çš„æ€»é•¿åº¦)
# speculative_disable_by_batch_size: Optional[int]
# ------------------------------------------------------------
model_path = "/PATH/TO/YOUR/Qwen2.5-14B-Instruct"
speculative_model_path = "/PATH/TO/YOUR/Qwen2.5-0.5B-Instruct"
python_executable = "/PATH/TO/YOUR/envs/bin/python"
# ------------------------------------------------------------

def run_vllm_server(port):
    vllm_server_command = None
    if port == "7777":
        # vanilla
        vllm_server_command = [
            python_executable, "-m", "vllm.entrypoints.openai.api_server",
            "--host", "0.0.0.0",
            "--port", str(port),
            "--model", model_path,
            "--dtype", "auto",
            #"--quantization", "awq_marlin",
            "--tensor-parallel-size", "1",                          
            "--gpu-memory-utilization", "0.9",
            "--disable-log-requests",
            #"--num-scheduler-steps", "4"   # å¯ä»¥ç¨ç¨æå‡ååé‡ï¼Œä½†ä¸å¤šï¼Œå› ä¸ºæœ¬æ¥ä¸¤ä¸ªtokenä¹‹é—´çš„ç©ºç™½é—´éš”å°±å¾ˆå°
        ]
        def _worker():
            os.environ["CUDA_VISIBLE_DEVICES"] = "4"  
            # ä½¿ç”¨ PyCUDA éœ€è¦å…ˆè°ƒç”¨ cuda.init() (åªéœ€è¦è°ƒç”¨ä¸€æ¬¡å³å¯)
            global pycuda_init_done
            if not pycuda_init_done:
                cuda.init()
                pycuda_init_done = True     
            dev = cuda.Device(0)
            ctx = dev.make_context()
            ctx.pop()
            ctx.detach()
            # æ¸…ç†å®Œæ¯•åï¼Œæ‰§è¡Œå¤–éƒ¨å‘½ä»¤
            os.execvp(python_executable, vllm_server_command)
        p = Process(target=_worker, daemon=False)
        p.start()
        return p
    
    else:
        # spec decoding
        vllm_server_command = [
            python_executable, "-m", "vllm.entrypoints.openai.api_server",
            "--host", "0.0.0.0",
            "--port", str(port),
            "--model", model_path,
            "--dtype", "auto",
            #"--quantization", "awq_marlin",
            "--tensor-parallel-size", "1",                          
            "--speculative-model", speculative_model_path,
            #"--speculative-model-quantization", "awq_marlin",
            "--gpu-memory-utilization", "0.9",
            "--use-v2-block-manager",
            "--num-speculative-tokens", "3",
            # ------ ğŸ¤” DEBUG ------
            "--disable-log-requests",
            #"--enable-chunked-prefill",   
            #"--scheduler-delay-factor", "0.5",      
            #"--num-scheduler-steps", "4",            #   File "vllm/engine/arg_utils.py", line 1138, in create_engine_config, ValueError: Speculative decoding is not supported with multi-step (--num-scheduler-steps > 1)
            "--speculative-max-model-len", "1280",
            #"--speculative-disable-by-batch-size", "8"
        ]
        def _worker():
            os.environ["CUDA_VISIBLE_DEVICES"] = "5"
            global pycuda_init_done
            if not pycuda_init_done:
                cuda.init()
                pycuda_init_done = True   
            dev = cuda.Device(0)
            ctx = dev.make_context()
            ctx.pop()
            ctx.detach()
            os.execvp(python_executable, vllm_server_command)
        p = Process(target=_worker, daemon=False)
        p.start()
        return p



def run_stress_test_client(task_name, dataset_path, client_script_path, port):
    
    client_command = [
        python_executable, client_script_path,
        "--backend", "openai-chat",
        "--base-url", f"http://localhost:{port}",  
        "--endpoint", "/v1/chat/completions",        
        "--dataset-name", task_name,      
        "--dataset-path", dataset_path,
        "--model", model_path,
        "--request-rate", "inf",       
        "--disable-tqdm",     
        "--seed", "12345",
        "--num-prompts", "120",    
        "--max-concurrency", "8"
    ]

    def _worker():
        os.execvp(python_executable, client_command)
    p = Process(target=_worker, daemon=False)
    p.start()
    return p

    
if __name__ == "__main__":
    dataset_path = "/PATH/TO/YOUR/datasets/Spec-Decode-Merged/cnn_merged.jsonl"  # æ¯ç§é•¿åº¦å„20æ¡
    client_script_path = "/PATH/TO/YOUR/[Round-3]stress_test_best_params/auto_test_code/[Round-3]benchmark_serving.py"
    TASK = "cnn_dailymail_news"   #"cnn_dailymail_highlights","cnn_dailymail_news", "THUCNews"
    PORTS = ["7777", "7778"]
    # å¹¶è¡Œå¯åŠ¨ä¸¤ä¸ª vllm æœåŠ¡
    vllm_processes = []
    for port in PORTS:
        vllm_process = run_vllm_server(port)
        vllm_processes.append((vllm_process, port))

    # ç­‰å¾…æœåŠ¡å¯åŠ¨
    time.sleep(200)
    for port in PORTS:
        try:
            r = requests.get(f"http://0.0.0.0:{port}/v1/models")
            if r.status_code == 200:
                print(f"============ å¼€å¯ vllm openai server, port={port} ============")
        except requests.exceptions.ConnectionError:
            raise TimeoutError("æœåŠ¡å¯åŠ¨å¤±è´¥")

    # å‹åŠ›æµ‹è¯•
    for task in [TASK]:
        client_processes = []
        for port in PORTS:
            print(f"\n\t ğŸ¥ å¼€å§‹å‘ port={port} å‘é€è¯·æ±‚, task={task}\n")

            process = run_stress_test_client(
                    task_name=task,
                    dataset_path=dataset_path,
                    client_script_path=client_script_path,
                    port=port
                )
            client_processes.append(process)
        
        # ç­‰å¾…æ‰€æœ‰clientå­è¿›ç¨‹å®Œæˆ
        for process in client_processes:
            process.join()

    # å…³é—­æ‰€æœ‰ vllm æœåŠ¡
    print(f"ğŸŒŸ ğŸŒŸ ğŸŒŸ æµ‹è¯•å·²å®Œæˆï¼Œç°åœ¨å…³é—­æ‰€æœ‰ vllm æœåŠ¡...")
    for vllm_process, port in vllm_processes:
        if vllm_process.is_alive():
            pid = vllm_process.pid  # åœ¨å…³ä¹‹å‰è·å– pid
            vllm_process.terminate() 
            vllm_process.join()
            print(f"å·²å…³é—­ vllm æœåŠ¡: pid={pid}, port={port}")

