<p align="center">
    <h1 align="center" style="margin-bottom:0px;">â±ï¸&nbsp;spec-bench-vllm&nbsp;â±ï¸</h1>
    <h6 align="center">Some tests on speculative decoding in vllm</h6>
</p>

<br>
**(update)âš ï¸ æ³¨æ„ï¼šæœ¬ä»“åº“ä¸­çš„å…¨éƒ¨åç»­æµ‹è¯•å‡åœ¨ vllm v0 ä¸Šè¿›è¡Œï¼Œæ‰€ä»¥ç»“æœä»…ä¾›å‚è€ƒï¼**

å‚è€ƒï¼š
- [spec-bench](https://github.com/hemingkx/Spec-Bench) ï¼šè¿™ä¸ªå·¥ä½œæµ‹è¯•äº†å„ç§æ¨æµ‹è§£ç æ–¹æ³•åœ¨ batchsize=1 ä¸‹ï¼Œåœ¨ä¸åŒä¸‹æ¸¸ä»»åŠ¡ä¸­ç›¸è¾ƒäºä¸ç”¨æ¨æµ‹è§£ç çš„åŠ é€Ÿæ¯”ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªåœ¨ç›¸åŒç¯å¢ƒä¸­å¯¹ç°æœ‰å¼€æºæ¨æµ‹è§£ç æ–¹æ³•è¿›è¡Œç³»ç»Ÿè¯„ä¼°çš„å¹³å°ã€‚ï¼ˆä½¿ç”¨å„ç§æ¨æµ‹è§£ç æ–¹æ³•çš„åŸå§‹ä»“åº“å®ç°ï¼‰
- [[vLLM vs TensorRT-LLM] #11. Speculative Decoding -Daehyun Ahn,Yeonjoon Jung](https://blog.squeezebits.com/vllm-vs-tensorrtllm-11-speculative-decoding-37301)ï¼šè¿™ä¸ªå¸–å­ä½¿ç”¨ Dynamic-Sonnet æ•°æ®é›†ã€Llama-3.1-70B-Struct/ Qwama-0.5B-Instruct æ¨¡å‹åœ¨ vllm å’Œ Tensorrt-llm æ¡†æ¶ä¸­æµ‹è¯•äº†åŸç‰ˆæ¨æµ‹è§£ç æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†è¯¦å°½çš„å›¾è¡¨å’Œæµ‹è¯•æ€è·¯ã€‚

## ğŸ“ æ¦‚è§ˆ

è¯¥æµ‹è¯•åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼Œ
- ç¬¬ä¸€é˜¶æ®µçš„æµ‹è¯•æ—¨åœ¨è¯„ä¼°æˆªæ­¢2025å¹´1æœˆçš„å„ç§æ¨æµ‹è§£ç æ–¹æ³•ï¼ˆåŸå§‹æ–¹æ³•ï¼ŒEagleï¼ŒMedusa ç­‰ï¼‰åœ¨ vllm ä¸­çš„è¡¨ç°
- ç¬¬äºŒé˜¶æ®µçš„æµ‹è¯•æ—¨åœ¨è¯„ä¼°æ¨æµ‹è§£ç åœ¨æ›´é•¿ä¸Šä¸‹æ–‡ã€ä¸åŒå‚æ•°é‡ç›®æ ‡æ¨¡å‹ã€é‡åŒ–ç›®æ ‡æ¨¡å‹ä¸­çš„è¡¨ç°
- ç¬¬ä¸‰é˜¶æ®µçš„æµ‹è¯•æ—¨åœ¨è¯„ä¼°æ¨æµ‹è§£ç åœ¨æ··åˆé•¿åº¦æ•°æ®é›†ä¸‹ï¼Œä¸åŒ--speculative-disable-by-batch-sizeã€ --speculative-max-model-len ç­‰ server å‚æ•°çš„è¡¨ç°

**æµ‹è¯•ç¯å¢ƒ**
- æ“ä½œç³»ç»Ÿï¼šRocky Linux 9.2 (Blue Onyx)
- cpu: Intel(R) Xeon(R) Platinum 8358P CPU @ 2.60GHz
- gpu: 1x NVIDIA A100-SXM4-80GB
- vllm 0.6.6 post1

## ğŸ“‚ æµ‹è¯•ä¸€ï¼šå„ç§æ¨æµ‹è§£ç æ–¹æ³•

è¯¥è½®æµ‹è¯•çš„ç›®çš„æ˜¯è¯„ä¼°æˆªæ­¢2025å¹´1æœˆå„ç§ä¸»æµæ¨æµ‹è§£ç æ–¹æ³•ï¼ˆåŸå§‹æ–¹æ³•ï¼ŒEagleï¼ŒMedusa ç­‰ï¼‰åœ¨ vllm ä¸­çš„è¡¨ç°ï¼Œä»£ç ä½äº [Round-1]stress_test_all_methods ä¸­ã€‚

### æ•°æ®é›†
- Spec-bench ä½¿ç”¨çš„æ•°æ®é›† ([link](https://github.com/hemingkx/Spec-Bench/blob/main/data/spec_bench/question.jsonl))ï¼Œä½äº datasets/spec-bench/
- Sharegpt çš„ä¸€ä¸ªå°å­é›†ï¼Œå…·æœ‰1024ä¸ªå•è½®å¯¹è¯æ ·æœ¬ï¼Œä½äº datasets/sharegpt_processed_1024.json

### æµ‹è¯•çš„æ–¹æ³•ï¼»æ³¨ï¼šæ‰€æœ‰æ–¹æ³•å‡ä½¿ç”¨ vllm çš„å®ç°ï¼Œè€ŒéåŸå§‹ä»“åº“ä¸­çš„å®ç°]
- eagle [[åŸå§‹ä»“åº“]](https://github.com/SafeAILab/EAGLE) [[vllm implementation]](https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/models/eagle.py) + Llama3-8b
- medusa [[åŸå§‹ä»“åº“]](https://github.com/FasterDecoding/Medusa) [[vllm implementation]](https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/models/medusa.py) + Vicuna-7b
- sps (åŸå§‹çš„æ¨æµ‹è§£ç æ–¹æ³•ï¼Œä½¿ç”¨ä¸€ä¸ªåŒç³»åˆ—å°æ¨¡å‹ä½œä¸ºè‰ç¨¿æ¨¡å‹) + Qwen2.5-7b
- pld (prompt lookup decoding) [[åŸå§‹ä»“åº“]](https://github.com/apoorvumang/prompt-lookup-decoding)+ Qwen2.5-7b
- vanilla (å•ç‹¬ä½¿ç”¨å¤§æ¨¡å‹ï¼Œä¸ä½¿ç”¨æ¨æµ‹è§£ç )

âš ï¸ æ³¨æ„ï¼šç”±äºæµ‹è¯•åœ¨ vllm æ¡†æ¶ä¸­è¿›è¡Œï¼Œvllm ä¸­è¿™äº›æ–¹æ³•çš„å®ç°å’ŒåŸå§‹å®ç°æœ‰ä¸€äº›åŒºåˆ«ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š
- âœ… vllm ä¸­æ‰€æœ‰çš„æ¨æµ‹è§£ç æ–¹æ³•å‡æ”¯æŒä»»æ„ batchsize çš„æ‰¹å¤„ç†
- âš ï¸ vllm 0.6.6 å°šæœªæ”¯æŒ Medusa å’Œ Eagle çš„ tree attentionï¼Œè¿™å¯¼è‡´ Medusa å’Œ Eagle åœ¨ vllm ä¸­çš„åŠ é€Ÿæ¯”ä½äºå®ƒä»¬çš„åŸå§‹ä»“åº“å®ç°

### Vllm å‚æ•°è¯´æ˜
**å…³äº MQA / Batch Expansion**

ç›®å‰ vllm å¹¶æœªå®ç° MQA kernelï¼Œå› æ­¤ä½¿ç”¨ Batch Expansion è¾¾åˆ°åœ¨ cuda graph ä¸­ä½¿ç”¨ MQA çš„ç›®çš„ã€‚

åœ¨æµ‹è¯•ä¸­ï¼Œåªæµ‹è¯•äº†ä½¿ç”¨ [Batch Expansion](https://github.com/vllm-project/vllm/blob/main/vllm/spec_decode/batch_expansion.py) çš„è®¾ç½®ï¼Œå¹¶æœªå¯ç”¨ [MQA](https://github.com/vllm-project/vllm/blob/main/vllm/spec_decode/mqa_scorer.py)ï¼ˆ--enforce-eagerï¼‰ï¼Œå› ä¸ºæ ¹æ®æµ‹è¯•ç»“æœï¼Œåœ¨ batchsize è¾ƒä½çš„åœºæ™¯ä¸‹ä½¿ç”¨ MQA çš„åŠ é€Ÿæ¯”æ˜æ˜¾ä½äºä½¿ç”¨ Batch Expansion çš„åŠ é€Ÿæ¯”ï¼Œè¿™ä¸ [pull#9298](https://github.com/vllm-project/vllm/pull/9298) çš„å®éªŒç»“æœä¸€è‡´ã€‚è€Œåœ¨ batchsize è¾ƒé«˜çš„åœºæ™¯ä¸‹ï¼Œè™½ç„¶ä½¿ç”¨ MQA ç›¸æ¯”ä½¿ç”¨ Batch Expansion æœ‰äº†åŠ é€Ÿï¼Œä½†æ­¤æ—¶ä½¿ç”¨æ¨æµ‹è§£ç çš„è¿è¡Œæ—¶é—´ä¼šå¤§äºä¸ä½¿ç”¨æ¨æµ‹è§£ç çš„è¿è¡Œæ—¶é—´ï¼ˆä½¿ç”¨ Batch Expansion è¿›è¡Œæ¨æµ‹è§£ç >ä½¿ç”¨ MQA è¿›è¡Œæ¨æµ‹è§£ç >>ä¸ä½¿ç”¨æ¨æµ‹è§£ç ï¼‰ã€‚


### æµ‹è¯•ç»“æœ

#### è¡¨1 - Max Values

*æ³¨*ï¼šä¸‹è¡¨ä¸­æ¯ä¸ªå•å…ƒæ ¼å†…çš„æ•°å€¼ä¸ºâ€œè¯¥æ–¹æ³•ç›¸æ¯”ä¸ç”¨æ¨æµ‹è§£ç çš„æ–¹æ³•ï¼Œåœ¨**ä¸åŒååé‡å’Œè‰ç¨¿ tokens æ•°é‡ä¸‹**èƒ½è¾¾åˆ°çš„**æœ€å¤§**åŠ é€Ÿæ¯”â€ï¼Œå¹¶ä¸å…·æœ‰ç»Ÿä¸€çš„å‚æ•°é™åˆ¶ï¼Œå› æ­¤è¯¥è¡¨æ ¼ä¸­çš„æ•°å€¼åªèƒ½åæ˜ ç†æƒ³è®¾ç½®ä¸‹çš„å³°å€¼æƒ…å†µï¼ŒçœŸå®æƒ…å†µä¸‹çš„åŠ é€Ÿæ¯”å¾€å¾€ä½äºæˆ–è¿œä½äºè¿™ä¸ªæ•°å€¼ï¼ˆè§æ¯ä¸ªæ–¹æ³•-ä¸‹æ¸¸ä»»åŠ¡çš„å…·ä½“çƒ­åŠ›å›¾ï¼‰ã€‚

è®¡ç®—æ–¹å¼ï¼š**åŠ é€Ÿæ¯” = æ¨æµ‹è§£ç æ–¹æ³•çš„`total_token_throughput` / ä¸ç”¨æ¨æµ‹è§£ç çš„`total_token_throughput`**ï¼Œthroughput ç›¸å…³æ•°å€¼å¯ä»¥åœ¨å‹åŠ›æµ‹è¯•è„šæœ¬ä¸­è·å¾—ï¼Œè¯¥å‹åŠ›æµ‹è¯•è„šæœ¬ä¿®æ”¹è‡ªï¼š[vllm/blob/main/benchmarks/benchmark_serving.py](https://github.com/vllm-project/vllm/blob/main/benchmarks/benchmark_serving.py)

å› ä¸ºåœ¨å®éªŒä¸­ï¼Œå‘ç° vllm ä¸­å³ä½¿è®¾ç½®äº†è´ªå©ªè§£ç ç›¸å…³å‚æ•°ï¼Œä¹Ÿæ— æ³•ä¿è¯æ¯æ¬¡åœ¨ç›¸åŒæ•°æ®é›†ä¸Šç”Ÿæˆçš„ tokens æ•°ä¸¥æ ¼ä¸€è‡´ï¼Œå®é™…ä¸Šå³ä½¿å•ç‹¬è¿è¡Œå¤§æ¨¡å‹ä¹Ÿä¼šå¯¼è‡´æ¯æ¬¡è¿è¡Œç”Ÿæˆçš„ tokens æ•°ä¸ä¸€æ ·ï¼ˆè§ [issue#5404](https://github.com/vllm-project/vllm/issues/5404) å’Œ [issue#6735](https://github.com/vllm-project/vllm/issues/6735)ï¼‰ ï¼Œæ‰€ä»¥ä¸ä½¿ç”¨`durationï¼ˆsecondï¼‰`è€Œæ˜¯ä½¿ç”¨`throughputï¼ˆtokens/secondï¼‰`æ¥è®¡ç®—åŠ é€Ÿæ¯”

<div align="center">
<img src="readme_imgs/1_max_values.png" width="90%">
</div>

#### è¡¨2 - Throughput Speedup Heatmap

å›¾ç‰‡ä¸ºæ¦‚è§ˆï¼Œå…³äºæ¯ä¸ªæ–¹æ³•åœ¨æ¯ä¸ªå­ä»»åŠ¡ä¸‹çš„å…·ä½“åŠ é€Ÿæ¯”å›¾åƒï¼Œå¯ä»¥åœ¨ auto_test_results/online çš„å„ä¸ªå­æ–‡ä»¶å¤¹é‡ŒæŸ¥çœ‹

<div align="center">
<img src="readme_imgs/1_heatmap_throughput.png" width="90%">
</div>

### ğŸ§· å¤ç°æµ‹è¯•ç»“æœ

1ã€å®‰è£… vllm==0.6.6

    pip install vllm==0.6.6

- å¦‚æœå’Œä»£ç ä¸­ä¸€æ ·ä½¿ç”¨ Qwen2.5 ç³»åˆ—æ¨¡å‹ï¼Œéœ€è¦å‚è€ƒè¿™ä¸ª [issue#5203](https://github.com/vllm-project/vllm/issues/5203) å°† vllm æºç ä¸­è‰ç¨¿æ¨¡å‹å’Œç›®æ ‡æ¨¡å‹çš„è¯è¡¨å¤§å°å¯¹é½ï¼Œvllm ç›®å‰åªæ”¯æŒè‰ç¨¿æ¨¡å‹å’Œç›®æ ‡æ¨¡å‹çš„è¯è¡¨å¤§å°ç›¸åŒ
- ç›®å‰æš‚æ—¶æ— æ³•åœ¨ vllm ä¸­ä½¿ç”¨ eagle æ–¹æ³•è¿è¡Œ qwen æ¨¡å‹

2ã€è¿›å…¥ç¬¬ä¸€è½®æµ‹è¯•çš„æ–‡ä»¶å¤¹ï¼Œç”Ÿæˆå­˜æ”¾æµ‹è¯•ç»“æœçš„æ–‡ä»¶ç›®å½•

    cd ./[Round-1]stress_test_all_methods/auto_test_code
    python init_file_dir.py

3ã€ä¿®æ”¹ `globals.py` é‡Œçš„æ•°æ®é›†è·¯å¾„ã€python æ‰§è¡Œæ–‡ä»¶è·¯å¾„ç­‰ï¼›ä¿®æ”¹ `main_{METHOD}.py` é‡Œçš„ `model_path`ã€`speculative_model_path` ä¸ºä½ çš„æœ¬åœ°æ¨¡å‹è·¯å¾„

è¿è¡Œ `main_{METHOD}.py` è„šæœ¬ä»¥ä½¿ç”¨ä¸åŒæ–¹æ³•è¿›è¡Œæµ‹è¯•ï¼Œè¯¥è„šæœ¬ä¼šé¦–å…ˆä½¿ç”¨ subprocess å¼€å¯ä¸€ä¸ª vllm æœåŠ¡çš„å­è¿›ç¨‹ï¼Œéšååœ¨ä¸åŒçš„ä»»åŠ¡æ•°æ®é›†ï¼ˆspec-bench æˆ– sharegptï¼‰ä¸Šè‡ªåŠ¨è¿è¡Œå‹åŠ›æµ‹è¯•ï¼Œç»Ÿè®¡åœ¨ä¸åŒå¹¶å‘æ•°ä¸‹çš„æ¨æµ‹è§£ç åŠ é€Ÿæ¯”ã€‚

*å› ä¸ºæ¯æ¬¡æ”¹å˜è‰ç¨¿ tokens æ•°é‡ï¼ˆ--num-spec-tokensï¼‰ä¼šå¯¼è‡´é‡æ–°å¼€å¯ vllm æœåŠ¡ï¼Œå› æ­¤å°†å®ƒæ”¾åœ¨äº†æœ€å¤–å±‚å¾ªç¯

    python main_{METHOD}.py

æµ‹è¯•ç»“æœé»˜è®¤ä¼šå­˜å…¥ init_file_dir.py é‡Œç”Ÿæˆçš„ `./auto_test_results/online/{METHOD}/{Task Name}/Table_MaxConcurrency.jsonl` è·¯å¾„ä¸‹ï¼Œå¯ä»¥åœ¨ `main_{METHOD}.py` è„šæœ¬ä¸­ä¿®æ”¹è¯¥è·¯å¾„ã€‚

4ã€ç»˜åˆ¶æµ‹è¯•ç»“æœ

    cd ../
    python [Round-1]draw_chart.py

è¿™ä¼šåœ¨æ¯ä¸ª `./auto_test_results/online/{METHOD}/{Task Name}` è·¯å¾„ä¸‹å„ç”Ÿæˆä¸€å¼ åæ˜ åŠ é€Ÿæ¯”çš„çƒ­åŠ›å›¾ï¼Œå¹¶ä¸”åœ¨ `./auto_test_results/online/max_speedup.jsonl` ä¸­è¿½åŠ è¯¥æ–¹æ³•åœ¨è¯¥æµ‹è¯•é›†ä¸Šèƒ½è¾¾åˆ°çš„æœ€å¤§åŠ é€Ÿæ¯”åŠå¯¹åº”å‚æ•°ï¼Œå¦‚ï¼š

    {"method": "eagle", "task": "ShareGPT", "num_spec_tokens": 2, "max_concurrency": 1, "speedup_ratio": 1.3179586746512673}

<br>
<hr size="10" color="slategrey">

## ğŸ“‚ æµ‹è¯•äºŒï¼šä¸åŒå‚æ•°é‡ç›®æ ‡æ¨¡å‹ã€ä¸åŒä¸Šä¸‹æ–‡é•¿åº¦

è¯¥è½®æµ‹è¯•çš„ç›®çš„æ˜¯è¯„ä¼°æ¨æµ‹è§£ç åœ¨æ›´é•¿ä¸Šä¸‹æ–‡ã€ä¸åŒå‚æ•°é‡ç›®æ ‡æ¨¡å‹ã€é‡åŒ–ç›®æ ‡æ¨¡å‹ä¸­çš„è¡¨ç°ï¼Œä»£ç ä½äº [Round-2]stress_test_sps_only ä¸­ã€‚

- å¹¶æœªå¯¹æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬è¿›è¡Œè´¨é‡è¯„ä¼°
- num_spec_tokens å›ºå®šä¸º3
- å…³äºé‡åŒ–æ–¹æ³•ï¼šæµ‹è¯•äº† GPTQ-int4 å’Œ AWQ-int4ï¼Œå‘ç° AWQ çš„è‰ç¨¿æ¥å—ç‡åœ¨ç°æœ‰ä»»åŠ¡ä¸‹ä¸€èˆ¬ç•¥é«˜äº GPTQï¼ˆé«˜0.05ï½0.09ä¸ç­‰ï¼‰ï¼Œå› æ­¤æœ€ç»ˆé€‰æ‹© AWQ

### æ¨¡å‹ä¸æ•°æ®é›†

è¯¥æµ‹è¯•åˆ†ä¸º3ä¸ªä¸‹æ¸¸ä»»åŠ¡ï¼š

- ç»™æ–°é—»ç”Ÿæˆæ‘˜è¦ï¼ˆå¤š prefillï¼Œå°‘ decodeï¼‰
  - prompt tokensï¼š["0.25k", "0.5k", "1k", "2k", "4k", "8k"]
  - output tokensï¼š2kåŠä»¥ä¸‹-128ï¼Œ4k-256ï¼Œ8k-512
  - æ•°æ®é›†ï¼šdatasets/CNN-DailyMail-News/

- ç»™æ‘˜è¦ç”Ÿæˆæ–°é—»ï¼ˆå°‘ prefillï¼Œå¤š decodeï¼‰
  - prompt tokensï¼š64ï½128
  - output tokensï¼š["0.25k", "0.5k", "1k", "2k", "4k", "8k"]
  - æ•°æ®é›†ï¼šdatasets/CNN-DailyMail-HighLights/

- æ–‡æœ¬æ¶¦è‰²ï¼ˆprefill ä¸ decode å­—æ•°å¤§è‡´ç›¸åŒï¼‰
  - prompt tokensï¼š["0.125k", "0.25k", "0.5k", "1k", "2k", "4k"]
  - output tokensï¼š["0.125k", "0.25k", "0.5k", "1k", "2k", "4k"]
  - æ•°æ®é›†ï¼šdatasets/THUCNews/

æ¨¡å‹å‡ä¸º Qwen 2.5-Instruct ç³»åˆ—

æ³¨1ï¼šå› ä¸º cnn-dailymail åŸå§‹æ•°æ®é›†çš„å•æ¡æ–°é—»é•¿åº¦æ™®éåœ¨ 2k ä»¥ä¸‹ï¼Œåœ¨ 4k åŠä»¥ä¸Šé•¿åº¦çš„æµ‹è¯•æ–‡ä»¶ä¸­ï¼Œå•æ¡æ ·æœ¬æ‰€åŒ…å«çš„æ–°é—»ä¸º2æ¡ä»¥ä¸ŠåŸå§‹æ•°æ®é›†ä¸­çš„æ–°é—»æ‹¼æ¥è€Œæˆ

æ³¨2ï¼šé€šè¿‡åœ¨ client å‘é€çš„ SamplingParams é‡ŒæŒ‡å®šâ€œmin_tokensâ€æ¥æŒ‡å®šæ¨¡å‹ä¸€æ¬¡æœ€å°‘ç”Ÿæˆå¤šå°‘å­—ç¬¦ï¼Œåœ¨ç”Ÿæˆè¿™äº›æ•°é‡çš„å­—ç¬¦ä¹‹å‰ä¸ä¼šç”ŸæˆEOS

### æµ‹è¯•ç»“æœ

âš ï¸ åœ¨æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬å‘ç°å³ä½¿æ˜¯ Qwen2.5-72B-Instruct ä¹Ÿæ€»æ˜¯åœ¨æŒ‡å®šç”Ÿæˆè¶…è¿‡ 1k tokensï¼ˆSamplingParams.min_tokens > 1000ï¼‰çš„æƒ…å†µä¸‹ç”Ÿæˆåƒåœ¾å­—ç¬¦ã€‚å› æ­¤æœ€ç»ˆå¹¶æ²¡æœ‰æµ‹è¯•[æ ¹æ®æ‘˜è¦ç”Ÿæˆæ–°é—»]çš„ä»»åŠ¡ï¼Œå¹¶ä¸”åœ¨æ–‡æœ¬æ¶¦è‰²ä»»åŠ¡ä¸­åªæµ‹è¯•äº†output tokensä¸º ["0.125k", "0.25k", "0.5k", "1k"] çš„ç»“æœã€‚

#### Throughput Speedup Heatmap

å›¾ç‰‡ä¸ºæ¦‚è§ˆï¼Œå…³äºæ¯ä¸ªæ–¹æ³•åœ¨æ¯ä¸ªå­ä»»åŠ¡ä¸‹çš„å…·ä½“åŠ é€Ÿæ¯”å›¾åƒï¼Œå¯ä»¥åœ¨ auto_test_results/online çš„å„ä¸ªå­æ–‡ä»¶å¤¹é‡ŒæŸ¥çœ‹

å…³äºæ¥å—ç‡ï¼štarget æ¨¡å‹å¢å¤§ï¼Œè‰ç¨¿æ¥å—ç‡ä¼šç¨ç¨é™ä½ï¼Œä½†æ€»ä½“è€Œè¨€ system efficiency éƒ½å¾ˆé«˜ï¼ˆ8kä¸Šä¸‹æ–‡çº¦0.5ï½0.6)

<div align="center">
<img src="readme_imgs/2_heatmap_throughput.png" width="90%">
</div>


### ğŸ§· å¤ç°æµ‹è¯•ç»“æœ

1ã€å®‰è£… vllm==0.6.6

    pip install vllm==0.6.6

- å¦‚æœå’Œä»£ç ä¸­ä¸€æ ·ä½¿ç”¨ Qwen2.5 ç³»åˆ—æ¨¡å‹ï¼Œéœ€è¦å‚è€ƒè¿™ä¸ª [issue#5203](https://github.com/vllm-project/vllm/issues/5203) å°† vllm æºç ä¸­è‰ç¨¿æ¨¡å‹å’Œç›®æ ‡æ¨¡å‹çš„è¯è¡¨å¤§å°å¯¹é½ï¼Œvllm ç›®å‰åªæ”¯æŒè‰ç¨¿æ¨¡å‹å’Œç›®æ ‡æ¨¡å‹çš„è¯è¡¨å¤§å°ç›¸åŒ
- ç›®å‰æš‚æ—¶æ— æ³•åœ¨ vllm ä¸­ä½¿ç”¨ eagle æ–¹æ³•è¿è¡Œ qwen æ¨¡å‹

2ã€è¿›å…¥ç¬¬äºŒè½®æµ‹è¯•çš„æ–‡ä»¶å¤¹ï¼Œç”Ÿæˆå­˜æ”¾æµ‹è¯•ç»“æœçš„æ–‡ä»¶ç›®å½•

    cd ./[Round-2]stress_test_sps_only/auto_test_code
    python init_file_dir.py

3ã€ä¿®æ”¹ `globals.py` é‡Œçš„æ•°æ®é›†è·¯å¾„ã€python æ‰§è¡Œæ–‡ä»¶è·¯å¾„ç­‰ï¼›ä¿®æ”¹ `example_{MODEL_TYPE}.py` é‡Œçš„ `model_path`ã€`speculative_model_path` ä¸ºä½ çš„æœ¬åœ°æ¨¡å‹è·¯å¾„

è¿è¡Œ `example_{MODEL_TYPE}.py` è„šæœ¬ä»¥ä½¿ç”¨ä¸åŒæ–¹æ³•è¿›è¡Œæµ‹è¯•ï¼Œè¯¥è„šæœ¬ä¼šé¦–å…ˆä½¿ç”¨ subprocess å¼€å¯ä¸€ä¸ª vllm æœåŠ¡çš„å­è¿›ç¨‹ï¼Œéšåä¾æ¬¡éå†ä¸åŒä»»åŠ¡-ä¸åŒä¸Šä¸‹æ–‡é•¿åº¦çš„æ•°æ®é›†ï¼Œè‡ªåŠ¨è¿è¡Œå‹åŠ›æµ‹è¯•ï¼Œç»Ÿè®¡åœ¨ä¸åŒå¹¶å‘æ•°ä¸‹çš„æ¨æµ‹è§£ç åŠ é€Ÿæ¯”ã€‚

    python example_{MODEL_TYPE}.py

æµ‹è¯•ç»“æœé»˜è®¤ä¼šå­˜å…¥ init_file_dir.py é‡Œç”Ÿæˆçš„ `./auto_test_results/online/{MODEL_TYPE}/{Task Name}/Table_MaxConcurrency.jsonl` è·¯å¾„ä¸‹ï¼Œå¯ä»¥åœ¨ `example_{MODEL_TYPE}.py` è„šæœ¬ä¸­ä¿®æ”¹è¯¥è·¯å¾„ã€‚

4ã€ç»˜åˆ¶æµ‹è¯•ç»“æœ

    cd ../
    python [Round-2]draw_chart.py

è¿™ä¼šåœ¨æ¯ä¸ª `./auto_test_results/online/{METHOD}/{Task Name}` è·¯å¾„ä¸‹å„ç”Ÿæˆä¸€å¼ åæ˜ åŠ é€Ÿæ¯”çš„çƒ­åŠ›å›¾ã€‚

<br>
<hr size="10" color="slategrey">

## ğŸ“‚ æµ‹è¯•ä¸‰ï¼šä¸åŒ server å‚æ•°

è¯¥è½®æµ‹è¯•çš„ç›®çš„æ˜¯è¯„ä¼°æ¨æµ‹è§£ç åœ¨æ··åˆé•¿åº¦æ•°æ®é›†ä¸‹ï¼Œä¸åŒæ¨æµ‹è§£ç ç›¸å…³ server å‚æ•°çš„è¡¨ç°ï¼Œä»£ç ä½äº [Round-3]stress_test_best_params ä¸­ã€‚

### æ¨¡å‹ä¸æ•°æ®é›†

- æ¨¡å‹ï¼šQwen2.5-14B-Instruct / Qwen2.5-0.5B-Instruct
- æ•°æ®é›†ï¼šä½äº`./datasets/Spec-Decode-Merged/cnn_merged.jsonl`ï¼Œè¯¥æ–‡ä»¶ä¸­æ¯20ä¸ªæ ·æœ¬ä¸ºä¸€ä¸ªé•¿åº¦ï¼Œä¾‹å¦‚1ï½20ä¸ªæ ·æœ¬ä¸º256ï¼Œ 21ï½40ä¸ªæ ·æœ¬ä¸º512... 100~120ä¸ªæ ·æœ¬ä¸º8kã€‚
  
### ğŸ§· è¿è¡Œæµ‹è¯•

1ã€å®‰è£… vllm==0.6.6

    pip install vllm==0.6.6

- å¦‚æœå’Œä»£ç ä¸­ä¸€æ ·ä½¿ç”¨ Qwen2.5 ç³»åˆ—æ¨¡å‹ï¼Œéœ€è¦å‚è€ƒè¿™ä¸ª [issue#5203](https://github.com/vllm-project/vllm/issues/5203) å°† vllm æºç ä¸­è‰ç¨¿æ¨¡å‹å’Œç›®æ ‡æ¨¡å‹çš„è¯è¡¨å¤§å°å¯¹é½ï¼Œvllm ç›®å‰åªæ”¯æŒè‰ç¨¿æ¨¡å‹å’Œç›®æ ‡æ¨¡å‹çš„è¯è¡¨å¤§å°ç›¸åŒ
- ç›®å‰æš‚æ—¶æ— æ³•åœ¨ vllm ä¸­ä½¿ç”¨ eagle æ–¹æ³•è¿è¡Œ qwen æ¨¡å‹

2ã€è¿›å…¥ç¬¬ä¸‰è½®æµ‹è¯•çš„æ–‡ä»¶å¤¹

    cd ./[Round-3]stress_test_best_params/manual_test_code

3ã€æµ‹è¯•æ–‡ä»¶ä½äº [NVTX]run_manual_stress_test.py ï¼Œè¯¥è„šæœ¬ä¼šä½¿ç”¨ multiprocessing åŒæ—¶å¼€å¯2ä¸ª vllm æœåŠ¡çš„è¿›ç¨‹ï¼Œä¸€ä¸ªç”¨æ¨æµ‹è§£ç ï¼Œä¸€ä¸ªä¸ç”¨æ¨æµ‹è§£ç ã€‚å¯ä»¥è°ƒè¯• `--speculative-disable-by-batch-size`ã€`.--speculative-max-model-len`ã€`--enable-chunked-prefill` ç­‰å‚æ•°

ä¸€ä¸ªå‚è€ƒ nsys å‘½ä»¤å¦‚ä¸‹ï¼š

    sudo -E nsys profile \
    --gpu-metrics-device="4,5" \
    --trace="cuda,nvtx" \
    --cuda-graph-trace="graph" \
    --cuda-memory-usage="true" \
    --delay="200" \
    --output="/PATH/TO/YOUR/Profile" \
    /PATH/TO/YOUR/python \
    "./[Round-3]stress_test_best_params/manual_test_code/[NVTX]run_manual_stress_test.py" 


